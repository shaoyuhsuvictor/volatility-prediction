%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 2.0 (February 7, 2023)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Author:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
%
% NOTE: The bibliography needs to be compiled using the biber engine.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
	a4paper,         % Paper size: a4paper or letterpaper
	12pt,            % Default font size
	unnumberedsections,  % Comment to enable section numbering
	twoside,         % Two-side layout
]{LTJournalArticle}
\usepackage{amsmath}
\usepackage{threeparttable}
\usepackage{booktabs}   % For professional looking tables
\usepackage{siunitx}    % Optional, for numeric alignment
\usepackage{graphicx}   % For \resizebox
\DeclareMathSizes{10}{10}{8}{7}
\addbibresource{reference.bib} % BibTeX bibliography file

\runninghead{High-Frequency Volatility Forecasting}
\setcounter{page}{1}
\title{
High-Frequency Volatility Forecasting and
Risk-Managed Trading Strategies in Cryptocurrency
Markets: A Gradient Boosting Approach
}

\date{Victor Hsu, Mackenzie Qu, Maria Kravchenko} 
% Full-width abstract (left empty)
\renewcommand{\maketitlehookd}{%
	\begin{abstract}
    We present a comprehensive machine learning framework for predicting realized volatility in cryptocurrency markets using high-frequency data. Our approach combines microstructure features from order book dynamics, trade flow imbalances, and price movements to forecast 60-minute ahead realized volatility of Bitcoin futures.  Using a LightGBM regressor optimized through walk-forward cross-validation, we achieve a strong out-of-sample Information Coefficient (IC) of 0.80. Furthermore, we implement a volatility-targeting strategy based on these forecasts. Backtesting results demonstrate that the volatility-managed portfolio yields superior risk-adjusted returns (Sharpe Ratio: 1.78) and significantly reduced maximum drawdown (-17.09\%) compared to a passive Buy \& Hold benchmark (-29.55\%).
	\end{abstract}
	\vspace{1ex}
}

\begin{document}
\maketitle
\section{Introduction}
Volatility forecasting is a cornerstone of quantitative finance, essential for risk management, option pricing, and portfolio construction. In cryptocurrency markets, characterized by regime shifts and extreme kurtosis, accurate volatility prediction is particularly challenging. While traditional econometric models (e.g., GARCH) capture autoregressive properties, they often fail to incorporate the rich information embedded in market microstructure.

This study proposes a supervised learning approach using Gradient Boosting Decision Trees (LightGBM) to forecast 1-hour ahead realized volatility for BTCUSDT perpetual futures. We hypothesize that incorporating Order Book Imbalance (OBI), Order Flow Imbalance (OFI), and trade flow dynamics enhances predictive power beyond simple autoregressive returns. Finally, we leverage these predictions to construct a dynamic volatility-targeting strategy that adjusts position sizing to stabilize portfolio variance.

%----- Create three subsections corresponding to the three prompts in the IAQF Student Problem 2025 -----
\section{Methodology}
\subsection{Data}The dataset comprises high-frequency market data for Binance BTCUSDT Futures, spanning the period from January 1, 2023, to December 31, 2024. The raw data consists of three distinct streams:

\begin{itemize}
    \item \textbf{Level 1 Data}: Best bid/ask prices, spreads, and tick-level volatility aggregated to 1-minute bars
    \item \textbf{Order Book Data}: Limit order book depth at various basis point levels from the mid-price (1, 3, 10, 30, 100 bps)
    \item \textbf{Trade Data}: Executed trades including volume, direction (buy/sell, taker/maker), and trade counts
\end{itemize}

All data streams were resampled to 1-minute intervals. Time series alignment was performed by taking the intersection of timestamps across all dataframes to ensure data integrity, resulting in a continuous timeline indexed by minute.

For missing observation due to various reasons, we implement a weighted training approach where carried-over observations receive reduced weight (0.5) compared to original data (1.0), while completely missing observations are removed. This preserves temporal continuity while appropriately downweighting less reliable data points.
\subsection{Feature Engineering}
We engineered a diverse set of features capturing price dynamics, liquidity, and market sentiment over multiple lookback windows $\tau \in \{1, 5, 15, 30, 60, 120\}$ minutes, detailed in Table \ref{tab:features}. The target variable is the annualized 1-hour forward realized volatility. 

\begin{table*}[ht!]
\centering
\caption{Feature Construction Definitions}
\label{tab:features}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllc@{}}
\toprule
\textbf{Feature Group} & \textbf{Feature Name(s)} & \textbf{Mathematical Formulation / Description} & \textbf{Time Scales ($\tau$)} \\ \midrule
\textbf{Target ($y_t$)} & $\text{Target\_RV}_{60m}$ & $\sigma_{t, \text{fwd}} = \sqrt{\frac{A}{h} \sum_{i=1}^{h} r_{t+i}^2}$, where $A=525,600$ (min/year) and $h=60$. & $60m$ \\ \midrule
\textbf{Return-Based} & $\text{log\_ret}_{\tau}$, $\text{abs\_ret}_{\tau}$, $\text{sq\_ret}_{\tau}$ & $\text{log\_ret}_{\tau} = \log(P_t / P_{t-\tau})$ & $\forall \tau \in \{1, 5, 15, 30, 60, 120\}$ \\
\textbf{Realized Volatility} & $\text{RV}_{\tau}$ & $\text{RV}_{\tau} = \text{std}_{\tau}(r_t) \cdot \sqrt{525600}$ & $\forall \tau \in \{1, 5, 15, 30, 60, 120\}$ \\
\textbf{Distribution Shape} & $\text{skew}_{\tau}$, $\text{kurtosis}_{\tau}$ & Third and fourth standardized moments of returns. & $\tau \geq 3$ \\ \midrule
\textbf{Order Book Imbalance (OBI)} & $\text{OBI}_{b}$ & $\frac{V^{\text{bid}}_b - V^{\text{ask}}_b}{V^{\text{bid}}_b + V^{\text{ask}}_b}$ & $b \in \{1, 3, 10, 30, 100\}$ bps \\
\textbf{Order Flow Imbalance (OFI)} & $\text{OFI}_{b,t}$ & $\frac{\Delta V^{\text{bid}}_{b,t} - \Delta V^{\text{ask}}_{b,t}}{\Delta V^{\text{bid}}_{b,t} + \Delta V^{\text{ask}}_{b,t}}$ & $b \in \{1, 3, 10, 30, 100\}$ bps \\ \midrule
\textbf{Trade Flow} & $\text{volume\_imb}$, $\text{count\_imb}$, EMA & Imbalances based on traded volume/count (buy vs sell), plus Exponential Moving Averages. & $1m$ base, various EMA scales \\
\textbf{Microstructure Noise} & $\text{spread\_norm}$, $\text{price\_range}$ & Normalized bid-ask spread and normalized high-low price range. & $1m$ \\
\textbf{Seasonality} & Time-based indicators & Cyclical encodings of hour/minute, DOW indicators. & N/A \\ \bottomrule
\end{tabular}%
}
\end{table*}

\subsection{Feature Selection}
To mitigate the curse of dimensionality and remove noise, we employed a sequential, two-step feature selection pipeline on the training data.

\subsubsection{Train-Validation-Test Split}
We utilized a Walk-Forward Validation approach with three folds for model training and validation to respect the temporal structure of financial data. A final Out-of-Sample (OOS) test period from July 1, 2024, to December 31, 2024, was reserved for final performance evaluation.

\subsubsection{Filtering \& Pruning}
The selection process involves two main stages: a predictive filter and a redundancy pruning step.

\paragraph{1. Predictive Filtering (Information Coefficient)}
Features are initially filtered based on their predictive power, measured by the Spearman Rank Correlation, or Information Coefficient (IC), between the feature $X$ and the target variable $Y$:
\begin{equation}
    \rho_S(X, Y) = \frac{\text{cov}(R_X, R_Y)}{\sigma_{R_X} \sigma_{R_Y}}
\end{equation}
where $R_X$ and $R_Y$ are the rank variables of $X$ and $Y$. We retain only features $X_i$ that satisfy an absolute IC threshold, $| \rho_S(X_i, Y) | \geq 0.10$. This step ensures all remaining features possess a statistically and economically significant monotonic relationship with the target.

\paragraph{2. Collinearity Pruning (Sequential Selection)}
The remaining features often exhibit high multicollinearity, which can destabilize tree-based models and obfuscate feature importance. We use the Pearson Correlation ($\rho_P$) to measure linear redundancy between features $X_i$ and $X_j$:
\begin{equation}
    \rho_P(X_i, X_j) = \frac{\sum (X_i - \mu_i)(X_j - \mu_j)}{\sqrt{\sum (X_i - \mu_i)^2 \sum (X_j - \mu_j)^2}}
\end{equation}
We apply a greedy, sequential selection algorithm \cite{featureselect}:
\begin{enumerate}
    \item Rank all filtered features $X$ in descending order based on their absolute Spearman IC \ref{fig:spearman_corr}.
    \item Initialize the set of selected features, $S = \emptyset$.
    \item Iterate through the ranked features $X_i$. If $X_i$ is highly correlated ($| \rho_P(X_i, X_j) | \geq 0.70$) with any feature $X_j \in S$, it is discarded. Otherwise, $X_i$ is added to $S$.
\end{enumerate}
This process ensures that the final feature set, $S$, is diverse while prioritizing those features with the strongest individual predictive capacity (highest IC).

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{fig/corrheatmap.png}
  \caption{Feature Correlation Matrix (Post-Pruning)}
    \label{fig:pearson_corr}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{fig/spearman_corr.png}
  \caption{Spearman Correlation Matrix (Post-Pruning)}
    \label{fig:spearman_corr}
\end{figure}

\subsection{Modeling Framework}
We utilized the LightGBM regressor \cite{ke2017}, a highly efficient gradient boosting framework capable of handling large-scale datasets. The model is built as an additive expansion of $K$ decision trees, $h_k(X)$, where $X$ is the input feature vector:

\begin{equation}
    F_K(X) = \sum_{k=1}^{K} \gamma_k h_k(X)
\end{equation}

The model minimizes a differentiable loss function $L(y, F(X))$ by iteratively fitting each new tree $h_k(X)$ to the negative gradient (pseudo-residuals) of the loss function with respect to the current model $F_{k-1}(X)$.

\subsubsection{Loss Function}
The loss function minimized during training was the Mean Squared Error (MSE), defined as:

\begin{equation}
    L(\mathbf{y}, \mathbf{\hat{y}}) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\end{equation}
where $y_i$ is the observed realized volatility and $\hat{y}_i = F(X_i)$ is the predicted volatility for the $i$-th observation, and $N$ is the total number of samples.

\subsubsection{Evaluation Metric}
While MSE guides the gradient optimization, the model selection and performance assessment relied on the Spearman Information Coefficient (IC). The IC, calculated as the rank correlation between predicted ($\hat{y}$) and actual ($y$) volatility, quantifies the model's ability to correctly rank the relative magnitude of future volatility regimes, which is critical for a volatility-targeting strategy:

\begin{equation}
    \text{IC} = \rho_S(\hat{y}, y) = \frac{\text{cov}(\text{Rank}(\hat{y}), \text{Rank}(y))}{\sigma_{\text{Rank}(\hat{y})} \sigma_{\text{Rank}(y)}}
\end{equation}
Hyperparameters (number of leaves, learning rate, and $n\_estimators$) were tuned using the average IC across the three validation folds.

\section{Results}
The final model was trained on data prior to July 2024 and evaluated on the subsequent test set. The model demonstrated robust predictive capability with the following metrics:
\begin{itemize}
    \item \textbf{RMSE:} 0.1996
    \item \textbf{MAE:} 0.1145
    \item \textbf{Spearman IC:} 0.8031
\end{itemize}

The high IC suggests the model is exceptionally good at ranking volatility regimes (i.e., distinguishing high volatility periods from low volatility periods), which is crucial for the trading strategy.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/pred_vs_actual_vol.png}
    \caption{Out-of-Sample Volatility Forecasts}
    \label{fig:pred_vs_actual}
\end{figure}

\subsection{Trade strategy:  Volatility Targeting}
We implemented a Volatility-Managed Portfolio strategy \cite{moreira2017}. The core premise is that asset returns inversely scaled by their volatility produce better risk-adjusted performance due to the "leverage effect" and clustering of variance. The strategy aims to maintain a constant level of risk exposure ($\sigma_{\text{target}}$) by dynamically adjusting the position size (weight $w_t$) inversely proportional to the predicted volatility ($\hat{\sigma}_t$).

The weight of the asset at time $t$, $w_t$, is determined by:
\begin{equation}
    w_t = \text{clip}\left( \frac{\sigma_{\text{target}}}{\hat{\sigma}_t}, w_{\min}, w_{\max} \right)
\end{equation}
Where:
\begin{itemize}
    \item $\sigma_{\text{target}}$ is the median realized volatility of the pre-test period (approx. 0.3191).
    \item $\hat{\sigma}_t$ is the predicted volatility for the next period.
    \item $w_{\min}=0.5$ and $w_{\max}=2.0$ are leverage constraints to prevent excessive risk-taking.
\end{itemize}

\subsection{Strategy backtest results}
The strategy was backtested on the OOS period using 1-minute rebalancing logic (applied to the 1-hour forecast horizon).

\begin{table}[ht]
\centering
\caption{Strategy Performance Comparison (OOS)}
\label{tab:performance}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Buy \& Hold} & \textbf{Vol-Managed Strategy} \\ \midrule
Total PnL & 0.4425 & 0.3078 \\
\textbf{Sharpe Ratio} & 1.6310 & \textbf{1.7765} \\
\textbf{Sortino Ratio} & 2.1726 & \textbf{2.5935} \\
\textbf{Max Drawdown} & -29.55\% & \textbf{-17.09\%} \\
Annualized Volatility & 54.02\% & 34.50\% \\ \bottomrule
\end{tabular}
}
\end{table}

While the raw PnL of the Buy \& Hold strategy was higher due to the strong directional trend of BTC during the period, the Volatility-Managed strategy significantly improved the quality of returns. The Max Drawdown was reduced by over 12 percentage points, and the Sharpe Ratio increased from 1.63 to 1.78.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\textwidth]{fig/strat.png}
    \caption{Cumulative Returns of Volatility Targeting Strategy}
    \label{fig:backtest}
\end{figure*}

\section{Limitations}

While the volatility-managed portfolio strategy demonstrates superior risk-adjusted returns in backtesting, the following real-world implementation constraints and model limitations must be rigorously addressed:

\begin{enumerate}
    \item \textbf{Transaction Costs and Market Slippage:} The current backtest assumes zero transaction costs and instant execution. In a high-frequency trading environment, the frequent rebalancing mandated by the volatility-managed strategy ($w_t$) would incur significant exchange fees and market slippage, particularly when adjusting large positions during periods of predicted high volatility (low liquidity). This could potentially erode the observed alpha.
    \item \textbf{Execution Latency and Data Staleness:} The model uses 1-minute bar data for feature calculation and assumes that the prediction $\hat{\sigma}_t$ is immediately actionable for the next period's returns. Real-world systems face inevitable execution latency. The delay means that the predicted volatility $\hat{\sigma}_t$ might be based on stale market microstructure data by the time the position adjustment is executed, degrading predictive value.
    \item \textbf{Model Stability and Feature Robustness:} The high out-of-sample IC suggests strong predictive power, but continuous monitoring of the model's stability is crucial. The model's reliance on highly granular microstructure features (OBI, OFI) may lead to performance degradation (concept drift) if market structures or trading behaviors shift.
\end{enumerate}

\section{Future Development}

To transition this research into a robust, deployable trading strategy, the following enhancements are proposed:

\begin{enumerate}
    \item \textbf{Cost-Agnostic Optimization:} Implement an optimization step to incorporate realistic transaction cost and slippage models. This involves solving for the optimal position size $w_t^*$ by maximizing the utility function:
    \begin{multline}
        w_t^* = \underset{w_t}{\arg \max} [ \mathbb{E}_t[r_{t+1} \cdot w_t] - \\
        \frac{1}{2}\gamma \cdot \text{Var}_t[r_{t+1} \cdot w_t] - \\C(w_t, w_{t-1}) ]
    \end{multline}
    where $C(w_t, w_{t-1})$ is the change in transaction cost incurred by rebalancing from $w_{t-1}$ to $w_t$, and $\gamma$ is the investor's risk aversion coefficient. This will determine the cost-efficient rebalancing frequency. Future work should incorporate a cost function into the objective, optimizing the trading strategy's speed and magnitude to maximize the Sharpe Ratio net of transaction costs, following advanced dynamic portfolio principles \cite{GarleanuPedersen2013}

    \item \textbf{Advanced Feature Validation and Pruning:} Conduct more rigorous validation, including Out-of-Time (OOT) testing and utilizing **Permutation Feature Importance** to confirm feature causality and prevent reliance on spurious correlations. This process should also involve explicitly modeling the decay rate of feature importance.

    \item \textbf{Dynamic Weight Management (Risk Parity):} Move beyond the fixed target volatility approach ($\sigma_{\text{target}}$). Instead, implement a Dynamic Risk Budgeting framework where the target volatility is itself a rolling, time-varying parameter calibrated to prevailing macroeconomic or sentiment conditions. This involves allocating capital based on the marginal risk contribution of the asset over time, potentially leveraging dynamic, time-consistent risk measures and deep learning techniques to adapt the risk profile based on changing market regimes \cite{Pesenti2025Risk}.
    
    \item \textbf{Ensemble Modeling:} The standalone LightGBM model could be combined with traditional, theoretically grounded time-series models (e.g., GARCH or HAR-RV) to form a hybrid ensemble. Combining the non-linear predictive power of the Machine Learning model with the strong time-series properties of classic volatility models is often shown to yield a more stable and accurate forecast across different market conditions \cite{Kim2018Hybrid}.
    \begin{equation}
        \hat{\sigma}_{\text{final}} = \lambda \hat{\sigma}_{\text{LGBM}} + (1-\lambda) \hat{\sigma}_{\text{HAR-RV}}
    \end{equation}
\end{enumerate}


\section{Summary}
This research demonstrates that Gradient Boosting models, fed with granular order book and trade flow features, can forecast Bitcoin volatility with high accuracy (IC $\approx$ 0.80). By utilizing these forecasts to dynamically size positions, we constructed a Volatility-Managed strategy that successfully smooths equity curves. The strategy reduces downside risk (Max Drawdown -17.09\%) and enhances risk-adjusted returns (Sharpe 1.78) compared to a passive holding strategy, validating the utility of machine learning in crypto-asset risk management.



%----------------------------------------------------------------------------------------
%	 REFERENCES
%----------------------------------------------------------------------------------------
\clearpage
\printbibliography

\clearpage % Start a new page
\appendix % Switch to appendix mode; section numbers will change accordingly

\end{document}
